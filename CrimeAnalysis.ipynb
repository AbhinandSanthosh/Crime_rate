{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a608c0a7-b3b4-4993-a269-fd7665a7e184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: missingno in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (0.5.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from missingno) (2.1.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from missingno) (3.10.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from missingno) (1.15.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from missingno) (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhinand\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->missingno) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhinand\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.17.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from seaborn->missingno) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abhinand\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn->missingno) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abhinand\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.2->seaborn->missingno) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e87c4c0-5a7d-45e3-922c-b23fa292f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba77c232-874f-40ea-bb59-3a8804119e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded. Shape: (40160, 16)\n",
      "‚úÖ Case closure rate: 50.0%\n",
      "‚úÖ Sample:\n",
      "        City Case Closed  case_closed_binary\n",
      "0  Ahmedabad           N                   0\n",
      "1    Chennai           N                   0\n",
      "2   Ludhiana           N                   0\n",
      "3       Pune           Y                   1\n",
      "4       Pune           Y                   1\n"
     ]
    }
   ],
   "source": [
    "# Load dataset - USING YOUR FILENAME\n",
    "df = pd.read_csv('crime_dataset_india.csv')  # ‚úÖ CORRECTED\n",
    "\n",
    "# Convert dates\n",
    "df['Date Reported'] = pd.to_datetime(df['Date Reported'], errors='coerce')\n",
    "df['Date of Occurrence'] = pd.to_datetime(df['Date of Occurrence'], errors='coerce')\n",
    "df['Date Case Closed'] = pd.to_datetime(df['Date Case Closed'], errors='coerce')\n",
    "\n",
    "# CRITICAL: Fix Case Closed column (handles concatenated strings)\n",
    "df['Case Closed'] = df['Case Closed'].astype(str).str[0]\n",
    "df['case_closed_binary'] = df['Case Closed'].str.lower().map({\n",
    "    'y': 1, 'yes': 1, '1': 1, 't': 1, 'true': 1,\n",
    "    'n': 0, 'no': 0, '0': 0, 'f': 0, 'false': 0\n",
    "})\n",
    "\n",
    "# Create regression target: days to close case\n",
    "df['days_to_close'] = (df['Date Case Closed'] - df['Date Reported']).dt.days\n",
    "\n",
    "print(f\"‚úÖ Data loaded. Shape: {df.shape}\")\n",
    "print(f\"‚úÖ Case closure rate: {df['case_closed_binary'].mean():.1%}\")\n",
    "print(f\"‚úÖ Sample:\\n{df[['City', 'Case Closed', 'case_closed_binary']].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8228a5f-5047-4b63-a451-d881640065de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features engineered. Shape: (40160, 28)\n"
     ]
    }
   ],
   "source": [
    "df_processed = df.copy()\n",
    "\n",
    "# 1. Datetime features\n",
    "df_processed['report_hour'] = df_processed['Date Reported'].dt.hour\n",
    "df_processed['report_dayofweek'] = df_processed['Date Reported'].dt.dayofweek\n",
    "df_processed['report_month'] = df_processed['Date Reported'].dt.month\n",
    "df_processed['report_year'] = df_processed['Date Reported'].dt.year\n",
    "\n",
    "# 2. Robust Time of Occurrence extraction\n",
    "if 'Time of Occurrence' in df_processed.columns:\n",
    "    df_processed['occurrence_hour'] = np.nan\n",
    "    \n",
    "    # Try regex for HH:MM format\n",
    "    mask_time = df_processed['Time of Occurrence'].astype(str).str.match(r'\\d{1,2}:\\d{2}')\n",
    "    df_processed.loc[mask_time, 'occurrence_hour'] = (\n",
    "        df_processed.loc[mask_time, 'Time of Occurrence']\n",
    "        .str.extract(r'(\\d{1,2}):')[0].astype(float)\n",
    "    )\n",
    "    \n",
    "    # Try parsing as datetime\n",
    "    mask_datetime = df_processed['occurrence_hour'].isna()\n",
    "    df_processed.loc[mask_datetime, 'occurrence_hour'] = pd.to_datetime(\n",
    "        df_processed.loc[mask_datetime, 'Time of Occurrence'], \n",
    "        errors='coerce'\n",
    "    ).dt.hour\n",
    "    \n",
    "    # Fallback: use report hour\n",
    "    df_processed['occurrence_hour'] = df_processed['occurrence_hour'].fillna(\n",
    "        df_processed['report_hour']\n",
    "    )\n",
    "    \n",
    "    # Drop original column\n",
    "    df_processed = df_processed.drop(columns=['Time of Occurrence'])\n",
    "\n",
    "# 3. Time delay feature\n",
    "df_processed['days_to_report'] = (df_processed['Date Reported'] - df_processed['Date of Occurrence']).dt.days\n",
    "\n",
    "# 4. Victim age groups (numeric codes)\n",
    "df_processed['victim_age_group'] = pd.cut(\n",
    "    df_processed['Victim Age'], \n",
    "    bins=[0, 18, 30, 50, 70, 100], \n",
    "    labels=[0, 1, 2, 3, 4]\n",
    ")\n",
    "\n",
    "# 5. Encode categoricals\n",
    "le_city = LabelEncoder()\n",
    "le_crime = LabelEncoder()\n",
    "le_weapon = LabelEncoder()\n",
    "le_domain = LabelEncoder()\n",
    "le_gender = LabelEncoder()\n",
    "\n",
    "df_processed['city_encoded'] = le_city.fit_transform(df_processed['City'].astype(str))\n",
    "df_processed['crime_code_encoded'] = le_crime.fit_transform(df_processed['Crime Code'].astype(str))\n",
    "df_processed['weapon_encoded'] = le_weapon.fit_transform(df_processed['Weapon Used'].astype(str))\n",
    "df_processed['domain_encoded'] = le_domain.fit_transform(df_processed['Crime Domain'].astype(str))\n",
    "df_processed['gender_encoded'] = le_gender.fit_transform(df_processed['Victim Gender'].astype(str))\n",
    "\n",
    "# 6. Text features\n",
    "df_processed['desc_word_count'] = df_processed['Crime Description'].astype(str).str.split().str.len()\n",
    "\n",
    "# 7. Remove problematic columns\n",
    "drop_cols = ['has_missing', 'date_for_split']\n",
    "df_processed = df_processed.drop(columns=[col for col in drop_cols if col in df_processed.columns])\n",
    "\n",
    "print(f\"‚úÖ Features engineered. Shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "560455ae-004c-4206-b6d9-45711ce7ac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selected 14 features\n",
      "\n",
      "‚úÖ Train: (10384, 14), Test: (5490, 14)\n"
     ]
    }
   ],
   "source": [
    "# EXCLUDE identifier and target columns\n",
    "exclude_cols = [\n",
    "    'Report Number', 'Date Reported', 'Date of Occurrence', 'Date Case Closed',\n",
    "    'Crime Description', 'City', 'Crime Code', 'Weapon Used', 'Crime Domain',\n",
    "    'Victim Gender', 'Case Closed', 'case_closed_binary', 'Police Deployed',\n",
    "    'days_to_close'\n",
    "]\n",
    "\n",
    "# Get final feature list\n",
    "feature_cols = [col for col in df_processed.columns if col not in exclude_cols]\n",
    "print(f\"‚úÖ Selected {len(feature_cols)} features\")\n",
    "\n",
    "# Targets\n",
    "y_class = df_processed['case_closed_binary']\n",
    "y_reg = df_processed['Police Deployed']\n",
    "\n",
    "# Time-based split\n",
    "split_date = '2023-01-01'\n",
    "train_mask = df_processed['Date Reported'] < split_date\n",
    "test_mask = df_processed['Date Reported'] >= split_date\n",
    "\n",
    "X_train = df_processed.loc[train_mask, feature_cols].copy()\n",
    "X_test = df_processed.loc[test_mask, feature_cols].copy()\n",
    "y_class_train = y_class[train_mask]\n",
    "y_class_test = y_class[test_mask]\n",
    "y_reg_train = y_reg[train_mask]\n",
    "y_reg_test = y_reg[test_mask]\n",
    "\n",
    "print(f\"\\n‚úÖ Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ff6b557-07a1-46d6-ac52-a25d9daa124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IMPUTATION DIAGNOSTICS ===\n",
      "Missing before: 0 cells\n",
      "‚úÖ KNN Imputation successful\n",
      "Missing after: 0 cells\n",
      "üíæ Models saved\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "print(\"=== IMPUTATION DIAGNOSTICS ===\")\n",
    "print(f\"Missing before: {X_train.isnull().sum().sum()} cells\")\n",
    "\n",
    "# 1. Convert boolean to int\n",
    "bool_cols = X_train.select_dtypes(include=['bool']).columns\n",
    "if len(bool_cols) > 0:\n",
    "    X_train[bool_cols] = X_train[bool_cols].astype(int)\n",
    "    X_test[bool_cols] = X_test[bool_cols].astype(int)\n",
    "\n",
    "# 2. Fix infinite values\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 3. Run imputation\n",
    "numeric_cols = X_train.columns.tolist()\n",
    "try:\n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    X_test_imputed = imputer.transform(X_test)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train_imputed, columns=numeric_cols, index=X_train.index)\n",
    "    X_test = pd.DataFrame(X_test_imputed, columns=numeric_cols, index=X_test.index)\n",
    "    print(\"‚úÖ KNN Imputation successful\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå KNN failed: {e}\\nFallback to Median...\")\n",
    "    simple_imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = simple_imputer.fit_transform(X_train)\n",
    "    X_test_imputed = simple_imputer.transform(X_test)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train_imputed, columns=numeric_cols, index=X_train.index)\n",
    "    X_test = pd.DataFrame(X_test_imputed, columns=numeric_cols, index=X_test.index)\n",
    "    imputer = simple_imputer\n",
    "\n",
    "print(f\"Missing after: {X_train.isnull().sum().sum()} cells\")\n",
    "\n",
    "# Save\n",
    "with open('imputer.pkl', 'wb') as f:\n",
    "    pickle.dump(imputer, f)\n",
    "\n",
    "encoders = {'city': le_city, 'crime': le_crime, 'weapon': le_weapon, \n",
    "            'domain': le_domain, 'gender': le_gender}\n",
    "with open('encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "\n",
    "print(\"üíæ Models saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e00248e1-6391-442c-9e4e-a84dc724261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Training SVM Classifier...\n",
      "\n",
      "=== SVM PERFORMANCE ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Open       0.49      1.00      0.66      2703\n",
      "      Closed       0.00      0.00      0.00      2787\n",
      "\n",
      "    accuracy                           0.49      5490\n",
      "   macro avg       0.25      0.50      0.33      5490\n",
      "weighted avg       0.24      0.49      0.32      5490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Training SVM Classifier...\")\n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "svm_model.fit(X_train, y_class_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_class_pred = svm_model.predict(X_test)\n",
    "y_class_prob = svm_model.predict_proba(X_test)\n",
    "\n",
    "print(\"\\n=== SVM PERFORMANCE ===\")\n",
    "print(classification_report(y_class_test, y_class_pred, target_names=['Open', 'Closed']))\n",
    "\n",
    "# Save\n",
    "with open('svm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef08d386-c0c7-4f8f-b0d5-aae485e012a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Training RandomForest Regressor...\n",
      "\n",
      "=== RANDOMFOREST PERFORMANCE ===\n",
      "MAE: 4.74 police officers\n",
      "R¬≤: -0.012\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Training RandomForest Regressor...\")\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=15, min_samples_split=5,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_reg_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_reg_pred = rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
    "r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "\n",
    "print(\"\\n=== RANDOMFOREST PERFORMANCE ===\")\n",
    "print(f\"MAE: {mae:.2f} police officers\")\n",
    "print(f\"R¬≤: {r2:.3f}\")\n",
    "\n",
    "# Save\n",
    "with open('rf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a706a05d-31e3-43a6-8ad5-f154a0f9b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Models and imputer saved with joblib!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Use the CORRECT variable names from your training cells\n",
    "joblib.dump(svm_model, 'svm_model.pkl')  # ‚úÖ svm_model, not svm\n",
    "joblib.dump(rf_model, 'rf_model.pkl')    # ‚úÖ rf_model, not rf\n",
    "joblib.dump(imputer, 'imputer.pkl')      # ‚úÖ This is correct\n",
    "\n",
    "print(\"\\n‚úÖ Models and imputer saved with joblib!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6746d-dd17-4887-9182-5a09eb211b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
